<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>00 Kubernetes架构</title>
    <url>/2024/08/31/00-Kubernetes%E6%9E%B6%E6%9E%84/</url>
    <content><![CDATA[<h1 id="Kubernetes架构"><a href="#Kubernetes架构" class="headerlink" title="Kubernetes架构"></a>Kubernetes架构</h1><p>Kubernetes的架构，由Master和Node两种节点组成，这两种角色分别对应控制节点和计算节点。</p>
<img src="/2024/08/31/00-Kubernetes%E6%9E%B6%E6%9E%84/k8s%E6%9E%B6%E6%9E%84%E5%9B%BE.png" class="" title="k8s架构图">

<p><strong>控制节点</strong>由三个紧密协作的独立组件组合而成，分别是负责API服务的kube-apiserver，负责调度的kube-scheduler，以及负责容器编排的kube-controller-manager。整个集群的持久化数据，则由kube-apiserver处理后保存在Etcd。</p>
<p><strong>计算节点</strong>上最核心的部分，则是一个叫作 kubelet 的组件。</p>
<p>在 Kubernetes 项目中，kubelet 主要负责同容器运行时（比如 Docker 项目）打交道。而这个交互所依赖的，是一个称作 CRI（Container Runtime Interface）的远程调用接口，这个接口定义了容器运行时的各项核心操作，比如：启动一个容器需要的所有参数。</p>
<p>这也是为何，Kubernetes 项目并不关心你部署的是什么容器运行时、使用的什么技术实现，只要你的这个容器运行时能够运行标准的容器镜像，它就可以通过实现 CRI 接入到 Kubernetes 项目当中。</p>
<p>而具体的容器运行时，比如 Docker 项目，则一般通过 OCI 这个容器运行时规范同底层的 Linux 操作系统进行交互，即：把 CRI 请求翻译成对 Linux 操作系统的调用（操作 Linux Namespace 和 Cgroups 等）。</p>
<p>此外，kubelet 还通过 gRPC 协议同一个叫作 Device Plugin 的插件进行交互。这个插件，是 Kubernetes 项目用来管理 GPU 等宿主机物理设备的主要组件，也是基于 Kubernetes 项目进行机器学习训练、高性能作业支持等工作必须关注的功能。</p>
<p>而kubelet 的另一个重要功能，则是调用网络插件和存储插件为容器配置网络和持久化存储。这两个插件与 kubelet 进行交互的接口，分别是 CNI（Container Networking Interface）和 CSI（Container Storage Interface）。</p>
<h1 id="核心功能全景图"><a href="#核心功能全景图" class="headerlink" title="核心功能全景图"></a>核心功能全景图</h1><img src="/2024/08/31/00-Kubernetes%E6%9E%B6%E6%9E%84/%E6%A0%B8%E5%BF%83%E5%8A%9F%E8%83%BD%E5%85%A8%E6%99%AF%E5%9B%BE.png" class="" title="核心功能全景图">

<p>Kubernetes 项目最主要的设计思想是，<strong>从更宏观的角度，以统一的方式来定义任务之间的各种关系，并且为将来支持更多种类的关系留有余地</strong>。</p>
<p>按照这幅图的线索，我们从容器这个最基础的概念出发，首先遇到了容器间“紧密协作”关系的难题，于是就扩展到了 Pod；有了 Pod 之后，我们希望能一次启动多个应用的实例，这样就需要 Deployment 这个 Pod 的多实例管理器；而有了这样一组相同的 Pod 后，我们又需要通过一个固定的 IP 地址和端口以负载均衡的方式访问它，于是就有了 Service。</p>
<p>可是，如果现在两个不同 Pod 之间不仅有“访问关系”，还要求在发起时加上授权信息。最典型的例子就是 Web 应用对数据库访问时需要 Credential（数据库的用户名和密码）信息。那么，在 Kubernetes 中这样的关系又如何处理呢？</p>
<p>Kubernetes 项目提供了一种叫作 Secret 的对象，它其实是一个保存在 Etcd 里的键值对数据。这样，你把 Credential 信息以 Secret 的方式存在 Etcd 里，Kubernetes 就会在你指定的 Pod（比如，Web 应用的 Pod）启动时，自动把 Secret 里的数据以 Volume 的方式挂载到容器里。这样，这个 Web 应用就可以访问数据库了。</p>
<p>除了应用与应用之间的关系外，应用运行的形态是影响“如何容器化这个应用”的第二个重要因素。</p>
<p>为此，Kubernetes 定义了新的、基于 Pod 改进后的对象。比如 Job，用来描述一次性运行的 Pod（比如，大数据任务）；再比如 DaemonSet，用来描述每个宿主机上必须且只能运行一个副本的守护进程服务；又比如 CronJob，则用于描述定时任务等等。</p>
<p>如此种种，正是 Kubernetes 项目定义容器间关系和形态的主要方法。</p>
<p>可以看到，Kubernetes 项目并没有像其他项目那样，为每一个管理功能创建一个指令，然后在项目中实现其中的逻辑。这种做法，的确可以解决当前的问题，但是在更多的问题来临之后，往往会力不从心。</p>
<p>相比之下，在 Kubernetes 项目中，我们所推崇的使用方法是：</p>
<p>首先，通过一个“编排对象”，比如 Pod、Job、CronJob 等，来描述你试图管理的应用；<br>然后，再为它定义一些“服务对象”，比如 Service、Secret、Horizontal Pod Autoscaler（自动水平扩展器）等。这些对象，会负责具体的平台级功能。<br>这种使用方法，就是所谓的“声明式 API”。这种 API 对应的“编排对象”和“服务对象”，都是 Kubernetes 项目中的 API 对象（API Object）。</p>
]]></content>
      <tags>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title>01 从0到1搭建Kubernetes集群</title>
    <url>/2024/08/31/01-%E4%BB%8E0%E5%88%B01%E6%90%AD%E5%BB%BAKubernetes%E9%9B%86%E7%BE%A4/</url>
    <content><![CDATA[<h1 id="环境准备"><a href="#环境准备" class="headerlink" title="环境准备"></a>环境准备</h1><h2 id="服务器配置"><a href="#服务器配置" class="headerlink" title="服务器配置"></a>服务器配置</h2><ul>
<li>每台机器 2 GB 或更多的 RAM（如果少于这个数字将会影响应用的运行内存）</li>
<li>CPU 2 核心及以上</li>
<li>集群中的所有机器的网络彼此均能相互连接（公网和内网都可以）</li>
<li>节点之中不可以有重复的主机名、MAC 地址（可以使用ip link或者ifconfig -a查看）或 product_uuid（可以使用sudo cat &#x2F;sys&#x2F;class&#x2F;dmi&#x2F;id&#x2F;product_uuid校验）</li>
</ul>
<table>
<thead>
<tr>
<th>主机名</th>
<th>ip</th>
</tr>
</thead>
<tbody><tr>
<td>node01（master）</td>
<td>192.168.180.100</td>
</tr>
<tr>
<td>node02（worker）</td>
<td>192.168.180.110</td>
</tr>
<tr>
<td>node03（worker）</td>
<td>192.168.180.120</td>
</tr>
</tbody></table>
<h2 id="环境配置"><a href="#环境配置" class="headerlink" title="环境配置"></a>环境配置</h2><p><strong>注意：如果没有特别说明，下面的指令都需要在三个节点上执行</strong></p>
<h3 id="关闭防火墙"><a href="#关闭防火墙" class="headerlink" title="关闭防火墙"></a>关闭防火墙</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">systemctl <span class="built_in">disable</span> firewalld &amp;&amp; systemctl stop firewalld</span><br></pre></td></tr></table></figure>

<h3 id="禁用selinux"><a href="#禁用selinux" class="headerlink" title="禁用selinux"></a>禁用selinux</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 将 SELinux 设置为 permissive 模式（相当于将其禁用）</span></span><br><span class="line">sudo setenforce 0</span><br><span class="line">sudo sed -i <span class="string">&#x27;s/^SELINUX=enforcing$/SELINUX=permissive/&#x27;</span> /etc/selinux/config</span><br></pre></td></tr></table></figure>

<p>通过运行命令 setenforce 0 和 sed … 将 SELinux 设置为 permissive 模式相当于将其禁用。 这是允许容器访问主机文件系统所必需的，例如，某些容器网络插件需要这一能力。 必须这么做，直到 kubelet 改进其对 SELinux 的支持。</p>
<h3 id="关闭交换分区"><a href="#关闭交换分区" class="headerlink" title="关闭交换分区"></a>关闭交换分区</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 暂时禁用交换分区</span></span><br><span class="line">sudo swapoff -a</span><br><span class="line"><span class="comment"># 配置重启后仍然禁用分区</span></span><br><span class="line">sed -i <span class="string">&#x27;s/.*swap.*/#&amp;/&#x27;</span> /etc/fstab</span><br><span class="line"><span class="comment"># 检查Swap</span></span><br><span class="line">free m</span><br><span class="line"></span><br><span class="line">              total        used        free      shared  buff/cache   available</span><br><span class="line">Mem:        4154356      126272     3871248       11916      156836     3808364</span><br><span class="line">Swap:             0           0           0</span><br></pre></td></tr></table></figure>

<h1 id="安装docker和containerd"><a href="#安装docker和containerd" class="headerlink" title="安装docker和containerd"></a>安装docker和containerd</h1><p>配置国内软件源</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">curl -o /etc/yum.repos.d/CentOS-Base.repo https://mirrors.aliyun.com/repo/Centos-7.repo</span><br><span class="line"></span><br><span class="line">yum makecache</span><br></pre></td></tr></table></figure>

<h2 id="安装docker"><a href="#安装docker" class="headerlink" title="安装docker"></a>安装docker</h2><p>配置docker仓库</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">vi /etc/yum.repos.d/docker-ce.repo </span><br><span class="line"></span><br><span class="line">[docker-ce-stable]</span><br><span class="line">name=Docker CE Stable - <span class="variable">$basearch</span></span><br><span class="line">baseurl=https://mirrors.aliyun.com/docker-ce/linux/centos/<span class="variable">$releasever</span>/<span class="variable">$basearch</span>/stable</span><br><span class="line">enabled=1</span><br><span class="line">gpgcheck=1</span><br><span class="line">gpgkey=https://mirrors.aliyun.com/docker-ce/linux/centos/gpg</span><br><span class="line"> </span><br><span class="line">[docker-ce-stable-debuginfo]</span><br><span class="line">name=Docker CE Stable - Debuginfo <span class="variable">$basearch</span></span><br><span class="line">baseurl=https://mirrors.aliyun.com/docker-ce/linux/centos/<span class="variable">$releasever</span>/debug-<span class="variable">$basearch</span>/stable</span><br><span class="line">enabled=0</span><br><span class="line">gpgcheck=1</span><br><span class="line">gpgkey=https://mirrors.aliyun.com/docker-ce/linux/centos/gpg</span><br><span class="line"> </span><br><span class="line">[docker-ce-stable-source]</span><br><span class="line">name=Docker CE Stable - Sources</span><br><span class="line">baseurl=https://mirrors.aliyun.com/docker-ce/linux/centos/<span class="variable">$releasever</span>/source/stable</span><br><span class="line">enabled=0</span><br><span class="line">gpgcheck=1</span><br><span class="line">gpgkey=https://mirrors.aliyun.com/docker-ce/linux/centos/gpg</span><br><span class="line"> </span><br><span class="line">[docker-ce-test]</span><br><span class="line">name=Docker CE Test - <span class="variable">$basearch</span></span><br><span class="line">baseurl=https://mirrors.aliyun.com/docker-ce/linux/centos/<span class="variable">$releasever</span>/<span class="variable">$basearch</span>/test</span><br><span class="line">enabled=0</span><br><span class="line">gpgcheck=1</span><br><span class="line">gpgkey=https://mirrors.aliyun.com/docker-ce/linux/centos/gpg</span><br><span class="line"> </span><br><span class="line">[docker-ce-test-debuginfo]</span><br><span class="line">name=Docker CE Test - Debuginfo <span class="variable">$basearch</span></span><br><span class="line">baseurl=https://mirrors.aliyun.com/docker-ce/linux/centos/<span class="variable">$releasever</span>/debug-<span class="variable">$basearch</span>/test</span><br><span class="line">enabled=0</span><br><span class="line">gpgcheck=1</span><br><span class="line">gpgkey=https://mirrors.aliyun.com/docker-ce/linux/centos/gpg</span><br><span class="line"> </span><br><span class="line">[docker-ce-test-source]</span><br><span class="line">name=Docker CE Test - Sources</span><br><span class="line">baseurl=https://mirrors.aliyun.com/docker-ce/linux/centos/<span class="variable">$releasever</span>/source/test</span><br><span class="line">enabled=0</span><br><span class="line">gpgcheck=1</span><br><span class="line">gpgkey=https://mirrors.aliyun.com/docker-ce/linux/centos/gpg</span><br><span class="line"> </span><br><span class="line">[docker-ce-nightly]</span><br><span class="line">name=Docker CE Nightly - <span class="variable">$basearch</span></span><br><span class="line">baseurl=https://mirrors.aliyun.com/docker-ce/linux/centos/<span class="variable">$releasever</span>/<span class="variable">$basearch</span>/nightly</span><br><span class="line">enabled=0</span><br><span class="line">gpgcheck=1</span><br><span class="line">gpgkey=https://mirrors.aliyun.com/docker-ce/linux/centos/gpg</span><br><span class="line"> </span><br><span class="line">[docker-ce-nightly-debuginfo]</span><br><span class="line">name=Docker CE Nightly - Debuginfo <span class="variable">$basearch</span></span><br><span class="line">baseurl=https://mirrors.aliyun.com/docker-ce/linux/centos/<span class="variable">$releasever</span>/debug-<span class="variable">$basearch</span>/nightly</span><br><span class="line">enabled=0</span><br><span class="line">gpgcheck=1</span><br><span class="line">gpgkey=https://mirrors.aliyun.com/docker-ce/linux/centos/gpg</span><br><span class="line"> </span><br><span class="line">[docker-ce-nightly-source]</span><br><span class="line">name=Docker CE Nightly - Sources</span><br><span class="line">baseurl=https://mirrors.aliyun.com/docker-ce/linux/centos/<span class="variable">$releasever</span>/source/nightly</span><br><span class="line">enabled=0</span><br><span class="line">gpgcheck=1</span><br><span class="line">gpgkey=https://mirrors.aliyun.com/docker-ce/linux/centos/gpg</span><br></pre></td></tr></table></figure>

<p>安装docker</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">yum install -y docker</span><br><span class="line">systemctl start docker &amp;&amp; systemctl <span class="built_in">enable</span> docker</span><br></pre></td></tr></table></figure>

<p>配置镜像加速</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">vi /etc/docker/daemon.json</span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">&quot;registry-mirrors&quot;</span>: [<span class="string">&quot;https://8740sp47.mirror.aliyuncs.com&quot;</span>]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>重启docker</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">systemctl daemon-reload</span><br><span class="line">systemctl restart docker</span><br></pre></td></tr></table></figure>

<h2 id="安装containerd"><a href="#安装containerd" class="headerlink" title="安装containerd"></a>安装containerd</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">yum install -y containerd</span><br><span class="line"><span class="built_in">mkdir</span> -p /etc/containerd</span><br><span class="line">containerd config default | sudo <span class="built_in">tee</span> /etc/containerd/config.toml</span><br></pre></td></tr></table></figure>

<p>修改配置</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">vi /etc/containerd/config.toml</span><br><span class="line"></span><br><span class="line"><span class="comment"># 修改或者添加这个选项</span></span><br><span class="line">  [plugins.<span class="string">&quot;io.containerd.grpc.v1.cri&quot;</span>.containerd.runtimes.runc.options]</span><br><span class="line">  SystemdCgroup = <span class="literal">true</span></span><br><span class="line"><span class="comment"># 修改此处替换成阿里云的源</span></span><br><span class="line">sandbox_image = <span class="string">&quot;registry.aliyuncs.com/google_containers/pause:3.7&quot;</span></span><br></pre></td></tr></table></figure>

<p>重启containerd</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">systemctl restart containerd</span><br><span class="line">systemctl <span class="built_in">enable</span> containerd</span><br></pre></td></tr></table></figure>



<h1 id="安装kubeadm"><a href="#安装kubeadm" class="headerlink" title="安装kubeadm"></a>安装kubeadm</h1><h2 id="配置Kubernetes的yum仓库"><a href="#配置Kubernetes的yum仓库" class="headerlink" title="配置Kubernetes的yum仓库"></a>配置Kubernetes的yum仓库</h2><p>在仓库定义中的 exclude 参数确保了与 Kubernetes 相关的软件包在运行 yum update 时不会升级，因为升级 Kubernetes 需要遵循特定的过程。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 此操作会覆盖 /etc/yum.repos.d/kubernetes.repo 中现存的所有配置</span></span><br><span class="line"><span class="built_in">cat</span> &lt;&lt;<span class="string">EOF | sudo tee /etc/yum.repos.d/kubernetes.repo</span></span><br><span class="line"><span class="string">[kubernetes]</span></span><br><span class="line"><span class="string">name=Kubernetes</span></span><br><span class="line"><span class="string">baseurl=https://mirrors.aliyun.com/kubernetes-new/core/stable/v1.30/rpm/</span></span><br><span class="line"><span class="string">enabled=1</span></span><br><span class="line"><span class="string">gpgcheck=1</span></span><br><span class="line"><span class="string">gpgkey=https://mirrors.aliyun.com/kubernetes-new/core/stable/v1.30/rpm/repodata/repomd.xml.key</span></span><br><span class="line"><span class="string">exclude=kubelet kubeadm kubectl cri-tools kubernetes-cni</span></span><br><span class="line"><span class="string">EOF</span></span><br></pre></td></tr></table></figure>

<h2 id="安装kubelet、kubeadm-和-kubectl"><a href="#安装kubelet、kubeadm-和-kubectl" class="headerlink" title="安装kubelet、kubeadm 和 kubectl"></a>安装kubelet、kubeadm 和 kubectl</h2><p>安装 kubelet、kubeadm 和 kubectl，并启用 kubelet 以确保它在启动时自动启动。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo yum install -y kubelet kubeadm kubectl --disableexcludes=kubernetes</span><br><span class="line">sudo systemctl <span class="built_in">enable</span> --now kubelet</span><br></pre></td></tr></table></figure>

<h1 id="创建Kubernetes集群"><a href="#创建Kubernetes集群" class="headerlink" title="创建Kubernetes集群"></a>创建Kubernetes集群</h1><h2 id="部署master节点"><a href="#部署master节点" class="headerlink" title="部署master节点"></a>部署master节点</h2><h3 id="kubeadm-init（只在主节点执行）"><a href="#kubeadm-init（只在主节点执行）" class="headerlink" title="kubeadm init（只在主节点执行）"></a>kubeadm init（只在主节点执行）</h3><p><strong>只在主节点执行</strong>，将下面的地址（192.168.180.100）修改为自己的地址</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">kubeadm init --pod-network-cidr=10.244.0.0/16 --apiserver-advertise-address 192.168.180.100 --image-repository registry.aliyuncs.com/google_containers</span><br><span class="line"></span><br><span class="line"><span class="comment"># 安装成功会看到如下信息</span></span><br><span class="line">Your Kubernetes control-plane has initialized successfully!</span><br><span class="line"></span><br><span class="line">To start using your cluster, you need to run the following as a regular user:</span><br><span class="line"></span><br><span class="line">  <span class="built_in">mkdir</span> -p <span class="variable">$HOME</span>/.kube</span><br><span class="line">  sudo <span class="built_in">cp</span> -i /etc/kubernetes/admin.conf <span class="variable">$HOME</span>/.kube/config</span><br><span class="line">  sudo <span class="built_in">chown</span> $(<span class="built_in">id</span> -u):$(<span class="built_in">id</span> -g) <span class="variable">$HOME</span>/.kube/config</span><br><span class="line"></span><br><span class="line">Alternatively, <span class="keyword">if</span> you are the root user, you can run:</span><br><span class="line"></span><br><span class="line">  <span class="built_in">export</span> KUBECONFIG=/etc/kubernetes/admin.conf</span><br><span class="line"></span><br><span class="line">You should now deploy a pod network to the cluster.</span><br><span class="line">Run <span class="string">&quot;kubectl apply -f [podnetwork].yaml&quot;</span> with one of the options listed at:</span><br><span class="line">  https://kubernetes.io/docs/concepts/cluster-administration/addons/</span><br><span class="line"></span><br><span class="line">Then you can <span class="built_in">join</span> any number of worker nodes by running the following on each as root:</span><br><span class="line"></span><br><span class="line">kubeadm <span class="built_in">join</span> 192.168.180.100:6443 --token 94ebi2.87y0jv2lg3soyymo \</span><br><span class="line">        --discovery-token-ca-cert-hash sha256:43aaea84c144c6d5ace987e932274ed510ed58f7394f187ce4797679276726eb</span><br></pre></td></tr></table></figure>

<p>继续执行下面的脚本，需要这些配置命令的原因是：Kubernetes 集群默认需要加密方式访问。所以，这几条命令，就是将刚刚部署生成的 Kubernetes 集群的安全配置文件，保存到当前用户的.kube 目录下，kubectl 默认会使用这个目录下的授权信息访问 Kubernetes 集群。如果不这么做的话，我们每次都需要通过 export KUBECONFIG 环境变量告诉 kubectl 这个安全配置文件的位置。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">mkdir</span> -p <span class="variable">$HOME</span>/.kube</span><br><span class="line">sudo <span class="built_in">cp</span> -i /etc/kubernetes/admin.conf <span class="variable">$HOME</span>/.kube/config</span><br><span class="line">sudo <span class="built_in">chown</span> $(<span class="built_in">id</span> -u):$(<span class="built_in">id</span> -g) <span class="variable">$HOME</span>/.kube/config</span><br><span class="line"><span class="built_in">export</span> KUBECONFIG=/etc/kubernetes/admin.conf</span><br></pre></td></tr></table></figure>

<h3 id="部署网络插件Flannel"><a href="#部署网络插件Flannel" class="headerlink" title="部署网络插件Flannel"></a>部署网络插件Flannel</h3><h4 id="加载Flannel镜像（三个节点都要执行）"><a href="#加载Flannel镜像（三个节点都要执行）" class="headerlink" title="加载Flannel镜像（三个节点都要执行）"></a>加载Flannel镜像（三个节点都要执行）</h4><p>通过kubectl检查节点上的系统pod的状态，我们可以看到coredns的状态是Pending，这是符合预期的，因为当前节点的网络尚未就绪，所以为了集群正常使用，需要部署网络插件。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl get pods -n kube-system</span><br><span class="line"></span><br><span class="line">NAME                             READY   STATUS    RESTARTS   AGE</span><br><span class="line">coredns-7b5944fdcf-6d4kr         0/1     Pending   0          13m</span><br><span class="line">coredns-7b5944fdcf-nbzth         0/1     Pending   0          13m</span><br><span class="line">etcd-node01                      1/1     Running   0          14m</span><br><span class="line">kube-apiserver-node01            1/1     Running   0          14m</span><br><span class="line">kube-controller-manager-node01   1/1     Running   0          14m</span><br><span class="line">kube-proxy-rmtm7                 1/1     Running   0          13m</span><br><span class="line">kube-scheduler-node01            1/1     Running   0          14m</span><br></pre></td></tr></table></figure>

<p>这里我们使用flannel，因为需要梯子，这里直接提供下载，解压安装包然后执行下面的指令即可</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">链接：https://pan.baidu.com/s/1eK0-K_plLJTKsA7f58FYFg </span><br><span class="line">提取码：0hms</span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 下载压缩包后解压</span></span><br><span class="line">unzip 安装k8s所需flannel必要镜像包.zip</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在解压后的目录执行</span></span><br><span class="line">ctr -n k8s.io i import flannel-cni-plugin-v1.1.2.tar</span><br><span class="line">ctr -n k8s.io i import flannel.tar</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看镜像</span></span><br><span class="line">crictl images | grep flannel</span><br><span class="line"></span><br><span class="line">docker.io/flannel/flannel-cni-plugin                              v1.1.2              7a2dcab94698c       8.25MB</span><br><span class="line">docker.io/flannel/flannel                                         v0.21.5             a6c0cb5dbd211       69.9MB</span><br></pre></td></tr></table></figure>

<h4 id="部署Flannel插件"><a href="#部署Flannel插件" class="headerlink" title="部署Flannel插件"></a>部署Flannel插件</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 执行脚本</span></span><br><span class="line">kubectl apply -f kube-flannel.yaml</span><br></pre></td></tr></table></figure>

<p>至此可以看到master节点已经就绪</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">kubectl get nodes</span><br><span class="line"></span><br><span class="line">NAME     STATUS   ROLES           AGE     VERSION</span><br><span class="line">node01   Ready    control-plane   3h17m   v1.30.4</span><br></pre></td></tr></table></figure>

<h2 id="部署worker节点"><a href="#部署worker节点" class="headerlink" title="部署worker节点"></a>部署worker节点</h2><p>安装好Flannel后，加入集群，在两个工作节点执行</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">kubeadm <span class="built_in">join</span> 192.168.180.100:6443 --token 94ebi2.87y0jv2lg3soyymo \</span><br><span class="line">        --discovery-token-ca-cert-hash sha256:43aaea84c144c6d5ace987e932274ed510ed58f7394f187ce4797679276726eb</span><br></pre></td></tr></table></figure>

<p>如果加入集群的命令找不到了可以在master节点重新生成一个</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">kubeadm token create --print-join-command</span><br></pre></td></tr></table></figure>

<h1 id="部署完成"><a href="#部署完成" class="headerlink" title="部署完成"></a>部署完成</h1><p>至此，Kubernetes集群已经部署完毕，可以检查集群状态</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 查看节点状态</span></span><br><span class="line">kubectl get nodes</span><br><span class="line"></span><br><span class="line">NAME     STATUS   ROLES           AGE     VERSION</span><br><span class="line">node01   Ready    control-plane   4h9m    v1.30.4</span><br><span class="line">node02   Ready    &lt;none&gt;          10m     v1.30.4</span><br><span class="line">node03   Ready    &lt;none&gt;          7m54s   v1.30.4</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看pods状态</span></span><br><span class="line">kubectl get pods -A</span><br><span class="line"></span><br><span class="line">NAMESPACE      NAME                             READY   STATUS    RESTARTS        AGE</span><br><span class="line">kube-flannel   kube-flannel-ds-5d9ws            1/1     Running   2 (7m59s ago)   8m30s</span><br><span class="line">kube-flannel   kube-flannel-ds-6lppl            1/1     Running   0               66m</span><br><span class="line">kube-flannel   kube-flannel-ds-pbpzn            1/1     Running   2 (10m ago)     11m</span><br><span class="line">kube-system    coredns-7b5944fdcf-6d4kr         1/1     Running   0               4h9m</span><br><span class="line">kube-system    coredns-7b5944fdcf-nbzth         1/1     Running   0               4h9m</span><br><span class="line">kube-system    etcd-node01                      1/1     Running   0               4h9m</span><br><span class="line">kube-system    kube-apiserver-node01            1/1     Running   0               4h9m</span><br><span class="line">kube-system    kube-controller-manager-node01   1/1     Running   0               4h9m</span><br><span class="line">kube-system    kube-proxy-lxvnl                 1/1     Running   0               11m</span><br><span class="line">kube-system    kube-proxy-rmtm7                 1/1     Running   0               4h9m</span><br><span class="line">kube-system    kube-proxy-vd424                 1/1     Running   0               8m30s</span><br><span class="line">kube-system    kube-scheduler-node01            1/1     Running   0               4h9m</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看集群状态</span></span><br><span class="line">kubectl get cs</span><br><span class="line"></span><br><span class="line">NAME                 STATUS    MESSAGE   ERROR</span><br><span class="line">controller-manager   Healthy   ok        </span><br><span class="line">scheduler            Healthy   ok        </span><br><span class="line">etcd-0               Healthy   ok  </span><br></pre></td></tr></table></figure>



<h1 id="文档参考"><a href="#文档参考" class="headerlink" title="文档参考"></a>文档参考</h1><blockquote>
<p>文档参考</p>
<p><a href="https://v1-30.docs.kubernetes.io/zh-cn/docs/setup/production-environment/tools/kubeadm/install-kubeadm/">https://v1-30.docs.kubernetes.io/zh-cn/docs/setup/production-environment/tools/kubeadm/install-kubeadm/</a><br><a href="https://blog.csdn.net/m0_67019144/article/details/138046111">https://blog.csdn.net/m0_67019144/article/details/138046111</a><br><a href="https://blog.csdn.net/qinxue722/article/details/140914287">https://blog.csdn.net/qinxue722/article/details/140914287</a></p>
<p><a href="https://kingdom.blog.csdn.net/article/details/140135764">https://kingdom.blog.csdn.net/article/details/140135764</a></p>
</blockquote>
]]></content>
      <tags>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title>传输控制协议TCP</title>
    <url>/2024/09/08/%E4%BC%A0%E8%BE%93%E6%8E%A7%E5%88%B6%E5%8D%8F%E8%AE%AETCP/</url>
    <content><![CDATA[<p>最近在看<a href="https://book.douban.com/subject/24740558/">计算机网络（第6版）</a>相关的基础知识，记录一下，做个TCP协议相关的笔记。</p>
<p>传输控制协议TCP(Transmission Control Protocol)是TCP&#x2F;IP体系中非常复杂的协议，笔记目录：</p>
<p>1、TCP的特点</p>
<p>2、在不可靠的网络上实现可靠传输的工作原理和实现</p>
<p>3、TCP的滑动窗口、流量控制、拥塞控制</p>
<p>4、TCP的连接管理</p>
<h1 id="TCP的特点"><a href="#TCP的特点" class="headerlink" title="TCP的特点"></a>TCP的特点</h1><p>1、<strong>TCP是面向连接的运输层协议</strong>。即应用程序在使用TCP协议前，需要先建立连接，在数据传输完毕之后，需要释放连接。就像打电话，需要先拨号建立连接，打完之后，需要挂断电话释放连接；</p>
<p>2、每一条连接只能有两个端点，即只能是点对点（一对一）；</p>
<p>3、TCP提供<strong>可靠交付</strong>的服务。通过TCP连接传送的数据，无差错、不丢失、不重复、按序到达；</p>
<p>4、TCP提供<strong>全双工通信</strong>。允许通信的双方的应用进程在任何时候都能发送数据。双方都设置有发送缓存和接收缓存。发送时，应用程序把数据传到发送缓存后，就可以做其他事了，TCP在合适的时候会把数据发送出去。在接收时，TCP会把数据先放到接收缓存，应用程序在合适的时候进行读取；</p>
<p>5、面向字节流。TCP中的“流”（stream）指的是流入到应用程序或从应用程序流出的字节序列。面向字节流的含义是虽然应用程序和TCP的交互是一次一个数据块（大小不等），但是TCP把应用程序交下来的数据看成仅仅是一连串的无结构的字节流。TCP并不知道所传送的字节流含义。TCP不保证应用程序所收到的数据块和发送应用程序所发出的数据块具有对应大小的关系（例如发送方用10个数据块发送，但是接收方可能只用了4个数据块就把收到的字节流交付给上层应用），但是保证接收方应用程序收到的字节流和发送方应用程序发出的字节流是一致的。</p>
<h1 id="可靠传输原理及实现"><a href="#可靠传输原理及实现" class="headerlink" title="可靠传输原理及实现"></a>可靠传输原理及实现</h1><p>TCP的报文段是交给IP层传送的，IP层是尽最大努力交付，并不是可靠的，因此TCP必须采取适当的措施才能使的两个运输层的通信变得可靠。</p>
<p>理想的传输条件有以下两个特点：</p>
<p>（1）传输信道不产生差错</p>
<p>（2）不管发送方以多块的速度发送数据，接收方都总是来得及处理收到的数据</p>
<p>但是现实中这种理想的传输条件是不存在的。因此我们需要做适当的措施保证通信的可靠。</p>
<h2 id="可靠传输原理"><a href="#可靠传输原理" class="headerlink" title="可靠传输原理"></a>可靠传输原理</h2><h3 id="停止等待协议"><a href="#停止等待协议" class="headerlink" title="停止等待协议"></a>停止等待协议</h3><p>“停止等待”就是每发送完一个分组就停止发送，等待对方的确认。在收到确认后在发送下一分组。</p>
<h4 id="无差错的情况"><a href="#无差错的情况" class="headerlink" title="无差错的情况"></a>无差错的情况</h4><p>下面左图中是最简单的情况，A发送分组M1，等待确认，B收到M1后向A确认，然后A收到确认后再发送M2，B收到后向A发送确认。M3也是类似的过程。</p>
<img src="/2024/09/08/%E4%BC%A0%E8%BE%93%E6%8E%A7%E5%88%B6%E5%8D%8F%E8%AE%AETCP/%E5%81%9C%E6%AD%A2%E7%AD%89%E5%BE%85%E5%8D%8F%E8%AE%AE1.png" class="" title="停止等待协议1">

<h4 id="出现差错"><a href="#出现差错" class="headerlink" title="出现差错"></a>出现差错</h4><p>上图的右图，B接收M1后检测到有差错就丢弃了M1，其他什么都不做（不通知M1）。也有可能M1在传输过程中丢失了，B自然也什么都不做。这两种情况，B都不会发送任何消息。</p>
<p>可靠传输是这样设计的：A只要超过一段时间仍然没有收到确认，就认为刚才发送的分组 丢失了，因而重传前面发送过的分组。这就是<strong>超时重传</strong>。要实现超时重传，就需要在发送完分组后设置一个<strong>超时计时器</strong>，如果超时没有收到确认就重发，如果在超时之前收到确认就撤销超时计时器。可以看到上图中的右图，A在发送M1后，超过一定时间没有收到B的确认，于是重新发送M1。</p>
<p>这里需要注意三点：</p>
<p>（1）A发送完一个分组后，必须暂时保留已发送的分组的副本（超时重传时使用）。收到确认后才能清除；</p>
<p>（2）分组和确认分组都必须进行编号。这样才能确认哪一个分组收到了确认，哪一个没有收到；</p>
<p>（3）超时计时器设置的重传时间应该比分组传输的平均往返时间更长一些。</p>
<h4 id="确认丢失和确认迟到"><a href="#确认丢失和确认迟到" class="headerlink" title="确认丢失和确认迟到"></a>确认丢失和确认迟到</h4><p>下图a中（确认丢失），B收到了M1，向A发送了确认，但是这个确认在传输过程中丢失了，A没有收到确认于是超时重传了M1，注意，B此时已经有了M1，因此B会丢弃重复的M1并向A发送确认。</p>
<img src="/2024/09/08/%E4%BC%A0%E8%BE%93%E6%8E%A7%E5%88%B6%E5%8D%8F%E8%AE%AETCP/%E5%81%9C%E6%AD%A2%E7%AD%89%E5%BE%85%E5%8D%8F%E8%AE%AE2.png" class="" title="停止等待协议2">

<p>上图b中（确认迟到），B收到M1后向A发送确认，但是因为网络原因，确认分组传输时间超过了超时重传的时间，于是A重新发送M1，</p>
<p>同样B收到重复的M1会丢弃并发送确认。A会收到重复的确认，此时A对重复确认的处理很简单：收到就丢弃。</p>
<p>使用上述的确认和重传机制，我们就可以在不可靠的传输网络上实现可靠的通信。这种可靠的传输协议通常称为自动重传请求ARQ（Automatic Repeat reQuest），意思是重传的请求是自动的，接收方不需要请求发送方重传某个出错的分组。</p>
<h3 id="连续ARQ协议"><a href="#连续ARQ协议" class="headerlink" title="连续ARQ协议"></a>连续ARQ协议</h3><p>停止等待协议优点是简单，但是缺点是信道利用率太低。所以为了提高传输效率，发送方可以不使用低效率的停止等待协议，而是采用流水线传输（如下图）。流水线传输一次可以连续发送多个分组，而不必每发完一个停顿下来等待对方确认。</p>
<img src="/2024/09/08/%E4%BC%A0%E8%BE%93%E6%8E%A7%E5%88%B6%E5%8D%8F%E8%AE%AETCP/%E6%B5%81%E6%B0%B4%E7%BA%BF%E4%BC%A0%E8%BE%93.png" class="" title="流水线传输">

<p>使用流水线传输时，就要使用到连续ARQ协议和滑动窗口（后面讲）。</p>
<p>下图a中，处于窗口中的5个分组都可以发送，而不需要等待对方确认。这样信道利用率就高了。</p>
<img src="/2024/09/08/%E4%BC%A0%E8%BE%93%E6%8E%A7%E5%88%B6%E5%8D%8F%E8%AE%AETCP/%E8%BF%9E%E7%BB%ADARQ%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86.png" class="" title="连续ARQ工作原理">

<p>连续ARQ协议规定，发送方每收到一个确认，就把发送窗口向前滑动一个分组的位置。上图b中，表示发送方收到了对第一个分组的确认，于是把发送窗口向前移动一个分组的位置。</p>
<p>接收方一般采用累计确认的方式。即接收方不必对每个分组逐个发送确认，而是在收到几个分组后，对<strong>按序到达的最后一个分组发送确认</strong>，表示到这个分组为止的所有分组都已正确收到了。</p>
<p>累计确认有优点也有缺点。优点：容易实现，即使确认丢失也不必重传（前面说过接收方会对按序到达的最后一个分组发送确认，如果第5个分组确认丢失了，但是后面发送方收到了第8个分组的确认，说明前8个分组都收到了，不用再重传第5个分组了）。缺点是不能向发送方反映出接收放已经正确收到的所有分组的信息。</p>
<p>例如，发送方发送了前5个分组，而中间的第3个分组丢失了。也就是接收方收到了第1，第2，第4，第5个分组，这是接收方只能对第1和第2个分组发出确认。发送方无法知道后面3个分组的下落，而只好把第3,4,5分组重新再传一次。即Go-back-N(回退N)。表示需要再退回来重传已发送的N个分组。</p>
<h2 id="TCP报文段的首部格式"><a href="#TCP报文段的首部格式" class="headerlink" title="TCP报文段的首部格式"></a>TCP报文段的首部格式</h2><p>TCP首部的前20字节是固定的，后面有4n字节是根据需要而增加的选项（n是整数）。因此TCP首部的最小长度是20字节。</p>
<img src="/2024/09/08/%E4%BC%A0%E8%BE%93%E6%8E%A7%E5%88%B6%E5%8D%8F%E8%AE%AETCP/TCP%E9%A6%96%E9%83%A8.png" class="" title="TCP首部">

<p>TCP首部固定部分各字段的意义：</p>
<ol>
<li><strong>源端口和目的端口</strong>各占2个字节，端口号最大为65535，0到1023为熟知端口号或系统端口号，1024到49151为登记端口号，49152~65535为短暂端口号（仅在客户进程运行时才动态选择）。</li>
<li><strong>序号</strong>占4个字节。序号范围是[0,2^32-1]。序号在增加到2^32-1之后，又回到0，也就是序号使用mod 2^32运算。TCP是面向字节流的，在一个TCP传送的字节流中，每一个字节都按顺序编号。首部中的序号字段值表示本报文段所发送的第一个字节的序号。如一报文段序号字段值是301，携带的数据共有100字节，表明本报文段第一个字节的序号为301，最后一个报文段的序号是400。如果还有下一个报文段，则下一个报文段的序号从401开始。这个字段也叫报文段序号。</li>
<li><strong>确认号</strong>占4个字节。期望收到对方下一个报文段的第一个数据字节的序号。假如B已经正确收到A发送过来的一个报文段，序号是300，数据为100字节，那么确认号就应该是401。</li>
<li><strong>数据偏移</strong>占4位。指出TCP报文段数据起始处距离TCP报文段起始处有多远，即TCP报文段的首部有多长。数据偏移的单位是32位字(4字节)，因为4位二进制最大的十进制为15，所以数据偏移最大的为60字节，首部固定最小20字节，选项长度最长不超过40字节。</li>
<li>保留占6位。保留为今后使用，但目前应置为0。</li>
<li><strong>紧急URG</strong>(URGent)。当URG&#x3D;1时表明紧急指针字段有效。当URG置1时，发送应用进程就告诉发送方的TCP有紧急数据要传送。于是发送方TCP就把紧急数据插入到本报文段数据的最前面，而在紧急数据后面的数据仍是普通数据。这时要与首部中紧急指针(Urgent Pointer)字段配合使用。</li>
<li><strong>确认ACK</strong>(ACKnowlegment)。仅当ACK&#x3D;1时，确认号字段才有效。TCP规定，在连接建立后所有传送的报文段都必须把ACK置为1。</li>
<li><strong>推送PSH</strong>(PuSH)。当两个应用进程进行交互式通信时，有时希望键入一个命令后马上得到对方的响应。这是发送方TCP将PSH置为1发送，接收方收到后就尽快交付应用进程，而不等到接收缓存都满了之后才交付应用进程。推送操作很少使用。</li>
<li><strong>复位RST</strong>(ReSeT)。当RST&#x3D;1时，表明TCP连接出现严重的差错（主机崩溃或其他原因），必须释放连接，然后重新建立连接。RST有时也用来拒绝非法报文段或拒绝打开一个连接。RST也可称为重建位或重置位。</li>
<li><strong>同步SYN</strong>(SYNchronization)用于在连接时用来同步序号。当SYN&#x3D;1而ACK&#x3D;0时，表明这是一个连接请求报文段。如果对方同意建立连接，则应响应SYN&#x3D;1和ACK&#x3D;1。</li>
<li><strong>终止FIN</strong>(FINis)用来释放一个连接。FIN&#x3D;1表明此报文段的发送方已发送完毕，请求释放连接。</li>
<li><strong>窗口</strong>占2字节。窗口指的是接收方的接收窗口。因为接收方的数据缓存空间是有限的，因此会有接收窗口限制。窗口值作为接收方让发送方设置其发送窗口的依据。</li>
<li><strong>校验和</strong>占2字节。校验和字段校验的范围包括首部和数据两部分。</li>
<li><strong>紧急指针</strong>占2字节。紧急指针仅在URG &#x3D; 1时才有意义，它指出本报文段中的紧急数据的字节数（紧急数据结束后就是普通数据）。因此，紧急指针指出了紧急数据的末尾在报文段中的位置。当所有紧急数据都处理完时，TCP就告诉应用程序恢复到正常操作。值得注意的是，即使窗口为零时也可发送紧急数据。</li>
<li><strong>选项</strong>长度可变。最长可达40字节。当没有使用“选项”时，TCP的首部长度是20字节。包含窗口扩大选项、时间戳选项和选择确认（SACK）选项。</li>
<li>填充字段。确保TCP首部为4字节的整数倍。</li>
</ol>
<h2 id="可靠传输的实现"><a href="#可靠传输的实现" class="headerlink" title="可靠传输的实现"></a>可靠传输的实现</h2><h3 id="以字节为单位的滑动窗口"><a href="#以字节为单位的滑动窗口" class="headerlink" title="以字节为单位的滑动窗口"></a>以字节为单位的滑动窗口</h3><p>TCP的滑动窗口是以字节为单位的。下图中根据B给的窗口值，A构造自己的发送窗口。有发送窗口就有接收窗口，这里先讨论发送窗口。</p>
<img src="/2024/09/08/%E4%BC%A0%E8%BE%93%E6%8E%A7%E5%88%B6%E5%8D%8F%E8%AE%AETCP/%E5%8F%91%E9%80%81%E7%AA%97%E5%8F%A3.png" class="" title="发送窗口">

<p><strong>发送窗口</strong>表示：在没有收到B的确认的情况下，A可以连续把窗口内的数据都发送出去。凡时发送出去的数据，在没有收到确认之前都必须保留，以便超时重传。</p>
<p>上图中，发送窗口内的序号是允许发送的序号，发送窗口越大，发送方就可以在得到确认之前发送越多数据，因而提高传输效率。前面说了，发送窗口是根据接收方的窗口值构造，因此发送窗口不能太大，需要考虑接收方是否来得及处理收到的数据。</p>
<p>后沿后面的数据表示已收到确认，不需要重新发送，保留的副本也可以清理了。</p>
<p>前沿前面的数据不能发送，因为接收方B还没有缓存可以接收这部分数据。</p>
<p>可以看到发送窗口是由前沿和后沿共同决定。</p>
<p>发送窗口的后沿的状态有两种：不动（没有收到新的确认）和前移（收到了新的确认）。<strong>不动</strong>：更加详细有两种情况，一是没有收到新的确认，或者收到的确认不是按序收到数据中最高序号的确认（比如收到的是32而不是31的确认，不会移动），且对方通知的窗口大小也没变；二是收到新的确认，但是对方通知的窗口变小了，前沿刚好不变。<strong>前移</strong>：一般后沿是不断前移的，不允许后移，因为不能撤销掉已收到的确认。</p>
<p>发送窗口的前沿也有可能向后收缩。这发生在对方通知窗口缩小了。但是<strong>TCP的标准强烈不赞成这样做</strong>。因为很可能在收到这个窗口值前，A已经发送了许多数据，现在又不让发送了，可能会导致一些错误产生。</p>
<h3 id="超时重传的时间选择"><a href="#超时重传的时间选择" class="headerlink" title="超时重传的时间选择"></a>超时重传的时间选择</h3><p>前面讲过，TCP的发送方在一定时间内没有收到确认，就会重传已发送的报文段。</p>
<p>超时重传的时间应该设置成多大？设置大了，网络资源空闲时间过长而浪费，设置小了，可能导致不必要的重传，所以需要探讨一个合适的超时重传时间。</p>
<p>TCP采用了一种自适应算法，它记录一个报文段发出的时间，以及收到相应报文段的确认的时间。这个两个时间之差就是<strong>报文段往返时间RTT</strong>。TCP保留了RTT的加权平均往返时间RRT<sub>S</sub>（又称平滑往返时间，S表示Smoothed）。当第一次测量到RTT时，RTT<sub>S</sub>等于RTT测量样本值，但是后面每测量到一个RTT样本，需要按照下面的公式重新计算一次RTT<sub>S</sub>：<br>$$<br>新的RTT_{S} &#x3D; (1-α)×(旧的RTT_{S})+α×(新的RTT样本)<br>$$<br>上式中，0≤α﹤1。若α接近于零，则新的RTT<sub>S</sub>相比旧的RTT<sub>S</sub>变化不大。若α接近于零，则在新的RTT<sub>S</sub>受RTT样本影响较大。</p>
<p>RFC 2988推荐的α值为1&#x2F;8，即0.125。用这种方法得出的加权平均往返时间RTTS就比测量出的RTT值更加平滑。</p>
<p><strong>超时重传时间RTO</strong>（RetransmissionTime-Out）应大于RTT<sub>S</sub>。RFC 2988建议使用下式计算RTO：<br>$$<br>RTO&#x3D;RTT_{S}+4×RTT_{D}<br>$$<br>RTT<sub>D</sub>是RTT的偏差的加权平均值，它与RTT<sub>S</sub>和新的RTT样本有关。RFC 2988建议这样计算RTT<sub>D</sub>。当第一次测量时，RTT<sub>D</sub>值取为测量到的RTT样本值的一半。在以后的测量中，则使用下式计算加权平均的RTT<sub>D</sub>：<br>$$<br>新的RTT_{D}&#x3D;(1-β)×(旧的RTT_{D})+β×|RTT_{S}-新的RTT样本|<br>$$<br>这里 β 是个小于1的系数，它的推荐值是1&#x2F;4，即0.25。</p>
<p>根据上面的三个公式就可以算出超时重传时间。我们发现，超时重传时间是很依赖RTT的，如果RTT计算不准确，则超时重传时间就会设置得不合适。</p>
<p>实际上，RTT的测量是比较复杂的。如下图，发送了一个报文段，设定的超时重传时间到了，还没有收到确认，于是重新发送一个报文段。经过一段时间后，收到确认。现在的问题是，这个确认是第一个报文段的确认，还是第二个重新发送报文段的确认？</p>
<img src="/2024/09/08/%E4%BC%A0%E8%BE%93%E6%8E%A7%E5%88%B6%E5%8D%8F%E8%AE%AETCP/%E5%BE%80%E8%BF%94%E6%97%B6%E9%97%B4%E7%A1%AE%E8%AE%A4.png" class="" title="往返时间确认">

<p>因此，<strong>Karn</strong>提出了一个算法：<strong>在计算平均RTT<sub>S</sub>时，只要报文段重传了，就不采用其往返的时间样本。这样得到的加权平均RTT<sub>S</sub>和RTO就较准确</strong>。</p>
<p>但是，这又引起新的问题：假如报文段的时延突然增大了许多，在原来得出的重传时间内，不会收到确认，于是需要重传报文段。根据上面Karn算法，发生重传不会采用其RTT样本，这样RTO就不能得到更新。</p>
<p>因此要对Karn算法进行修正。方法是：报文段每重传一次，就把超时重传时间RTO增大一些。典型的做法是取新的重传时间为2倍的旧的重传时间。当不再发生报文段的重传时，才根据上面给出的公式计算超时重传时间。实践证明，这种策略较为合理。</p>
<h3 id="选择确认SACK"><a href="#选择确认SACK" class="headerlink" title="选择确认SACK"></a>选择确认SACK</h3><p>有一个场景：若是收到的报文段无差错，只是未按序到达，中间还缺少一些序号的数据，那么能都只重传中间缺少的数据。比如接受方缓存收到了1<del>3号字节块和7</del>10号字节块，中间的4~6号字节块丢失了，能否只传丢失的字节块？</p>
<p>答案是可以的，使用选择确认。</p>
<p>但是因为SACK文档并没有指明发送方应当怎样响应SACK，因此现实中，大多数实现还是重传未被确认的数据块。</p>
<h1 id="TCP的流量控制"><a href="#TCP的流量控制" class="headerlink" title="TCP的流量控制"></a>TCP的流量控制</h1><h2 id="利用滑动窗口实现流量控制"><a href="#利用滑动窗口实现流量控制" class="headerlink" title="利用滑动窗口实现流量控制"></a>利用滑动窗口实现流量控制</h2><p>一般来说，我们总是希望数据传输得快一些。但是如果发送方发送过快，可能会导致接收方来不及接收，造成数据丢失。所谓<strong>流量控制（flow control）就是让发送方的发送速率不要太快，要让接收方来得及接收</strong>。</p>
<p>滑动窗口机制可以很方便地在TCP连接上实现对发送方的流量控制。</p>
<p>如下图中，A向B发送数据。在建立连接时，B告诉了A（下图中没有表现出来）：“我的接收窗口是rwnd&#x3D;400“（rwnd表示receiver window）。因此，<strong>发送方的发送窗口不能接收方给出的接收窗口的数值</strong>。需要注意，<strong>TCP的窗口单位是字节，不是报文段</strong>。</p>
<p>下图中， 接收方的主机B进行了三次流量控制，第一次把窗口减小到rwnd&#x3D;300，第二次rwnd&#x3D;100，第三次rwnd&#x3D;0，即不允许发送方再发送数据。这种发送方暂停发送数据的状态一直到接收方主机B发送一个新的窗口值为止。</p>
<img src="/2024/09/08/%E4%BC%A0%E8%BE%93%E6%8E%A7%E5%88%B6%E5%8D%8F%E8%AE%AETCP/%E6%BB%91%E5%8A%A8%E7%AA%97%E5%8F%A3%E8%BF%9B%E8%A1%8C%E6%B5%81%E9%87%8F%E6%8E%A7%E5%88%B6.png" class="" title="滑动窗口进行流量控制">

<p>现在有这样一种情况：B向A发送零窗口报文段不久，B的接收缓存又有了一些空间，这时B向A发送了rwnd&#x3D;400的报文段。然而这个报文段在传送过程中丢失了。这时候A因为没有收到新的窗口而一直处于阻塞状态，而B一直等待A发送数据，从而导致死锁。</p>
<p>为了解决这个问题，TCP为每个连接设有一个<strong>持续计时器</strong>（persistence timer）。只有TCP连接的一方收到零窗口，就启动持续计时器，若计时器到期，就发送一个零窗口探测报文段（仅携带1字节的数据），对方就在这个确认这个探测报文段时给出现在的窗口值（TCP规定，即使设置了零窗口，也必须接收以下几种报文段：零窗口探测报文段、确认报文段、携带紧急数据的报文段）。如果窗口仍然是零，那么重新设置持续计时器，如果不是零，死锁僵局被打破，可以正常发送数据了。</p>
<h2 id="必须考虑传输效率"><a href="#必须考虑传输效率" class="headerlink" title="必须考虑传输效率"></a>必须考虑传输效率</h2><p>应用进程把数据传送到TCP的发送缓存后， 剩下的发送任务就由TCP来控制了。</p>
<p>可以有不同的机制来控制TCP报文段发送的时机：</p>
<p>第一种，TCP维持一个变量，它等于最大报文段长度MSS，只要缓存中的数据MSS字节时，就组装成一个TCP报文段发送出去。</p>
<p>第二种，由应用进程指明要求发送报文段，即TCP支持推送（push）操作。</p>
<p>第三种，发送方的计时器期限到了，就把当前已有的缓存发送出去（不能超过MSS）。</p>
<p>在TCP的实现中广泛使用<strong>Nagle算法</strong>。算法如下：</p>
<p>若发送应用进程把要发送的的数据逐个字节地送到TCP缓存，则发送方就把第一个数据字节先发送出去，把后面到达的数据字节缓存起来；</p>
<p>当发送方收到对第一个数据字符的确认后，再把发送缓存中的所有数据组成一个报文段发送出去，同时继续对随后到达的数据进行缓存；</p>
<p>只有收到前一个报文段的确认才继续发送下一个报文段；</p>
<p>当到达的数据已达到发送窗口大小的一半或达到报文段的最大长度时，就立即发送一个报文段。这样可以有效提高网络吞吐量。</p>
<p>以上就是Nagel算法。</p>
<p>还有一个问题是<strong>糊涂窗口综合症</strong>（silly window syndrome），有时也会使TCP性能变坏。有这样一种情况，TCP的接收方的缓存已满，而交互式的应用进程一次只从缓存中读取一个字节（这样接收缓存只腾出一个字节的空间），然后向发送方发出确认，并把窗口设置成1个字节。接着发送方又发来1字节数据。接收方又发出确认，窗口仍为1。这样，网络的使用率就会很低。</p>
<p>要解决这个问题，可以让 接收方等待一段时间，使得接收方缓存已有足够空间容纳一个最长的报文段，或者等到接收方缓存已有一半空闲的空间。只要出现这两种情况之一，接收方就发出确认报文，并向对方通知当前窗口大小。此外，发送方也不要发送太小的报文段，而是把数据累计成足够大的报文段，或达到接收方缓存的空间的一半大小。</p>
<p>上述方法可配合使用。使得发送方不发送很小的报文段，接收方也不要刚有一点缓存空间就急忙把这个很小的窗口通知对方。</p>
<h1 id="TCP的拥塞控制"><a href="#TCP的拥塞控制" class="headerlink" title="TCP的拥塞控制"></a>TCP的拥塞控制</h1><h2 id="拥塞控制的一般原理"><a href="#拥塞控制的一般原理" class="headerlink" title="拥塞控制的一般原理"></a>拥塞控制的一般原理</h2><p>在计算机网络中，链路容量（带宽）、交换结点中的缓存和处理机等，都是网络的资源。在某段时间内，若对网络中某一资源的需求超过了该资源所能提供的可用部分，网络的性能可能就要变坏，这种情况就叫<strong>拥塞</strong>。</p>
<p>为了解决拥塞问题，是不是只要任意增加一些资源就可以了？比如把结点的缓存增大，把链路更换为更高速率的链路，把结点的处理机运算速度提高等。因为网络拥塞是一个非常复杂的问题，简单地按照上面的做法，许多情况下问题不但不能解决，可能网络性能可能更坏。</p>
<p>网络拥塞往往是由许多因素引起的。当某个结点的缓存容量太小时，到达该结点的分组因为无法缓存而丢弃。现在假如该结点扩展了缓存容量，可以存放更多的分组。于是凡时到达该结点的分组都可以在该结点排队，不受任何限制。由于链路的速率并没有提高，分组在该结点排队时间远远超过了超时重传时间，于是上层程序重传分组，造成了更大的拥塞，网络性能变得更差了。</p>
<p>那如果是结点的处理机速度太慢导致拥塞呢？简单地将处理机速度提升，可能会缓解丢包情况，但是瓶颈往往又转移到另外的地方。</p>
<p>因此，<strong>网络拥塞的问题实质是各部分不匹配，只有各部分都平衡了，问题才会得到解决</strong>。</p>
<p>拥塞控制和流量控制的区别：</p>
<p>拥塞控制是防止过多数据注入到网络中，是全局性的过程。</p>
<p>流量控制是点对点通信的控制，是个端到端的问题。</p>
<p>比如，有一条光纤网络的链路传输速率为1000Gb&#x2F;s。有一个巨型计算机以1Gb&#x2F;s向一个PC机发送数据，此时不会产生拥塞因为网络带宽是足够的，但是流量控制是需要的，因为PC机可能来不及接收。</p>
<p>但是如果光纤网络的链路传输速率为1Mb&#x2F;s，有1000台巨型计算机连接在这个网络上，其中500台以100kb&#x2F;s向另外500台发送数据，这时候，巨型计算机不用考虑来得及接收，而是要考虑整个网络的输入是否超过了网络的负载。</p>
<p>从原理来讲无非就是使对资源的请求小于可用资源。或增加网络资源，或减少用户需求。如前面说的，当采用某种措施时，需要整体考虑。</p>
<p>从大的方面看，有两种方法看拥塞问题。</p>
<p><strong>开环控制</strong>：在设计网络时事先将有关发生拥塞的因素考虑周到，力求网络在工作时不产生拥塞。但一旦整个系统运行起来就不允许修改了。</p>
<p><strong>闭环控制</strong>：闭环控制是基于反馈环路的概念。有如下几个措施：</p>
<p>（1）监测网络系统以便检测拥塞发生在何时、何处</p>
<p>（2）把拥塞发生的信息发送到可采取行动的地方</p>
<p>（3）调整网络系统的云心以解决出现的问题</p>
<h2 id="几种拥塞控制的方法"><a href="#几种拥塞控制的方法" class="headerlink" title="几种拥塞控制的方法"></a>几种拥塞控制的方法</h2><p>1999年公布的因特网建议标准RFC 2581定义了进行拥塞控制的四种算法。<strong>慢开始</strong>（slow-start）、<strong>拥塞避免</strong>（congestion avoidance）、<strong>快重传</strong>（fast retransmit）和<strong>快恢复</strong>（fast recovery）。以后RFC 2582和RFC 3390又对这些算法进行了一些改进。</p>
<h3 id="慢开始和拥塞避免"><a href="#慢开始和拥塞避免" class="headerlink" title="慢开始和拥塞避免"></a>慢开始和拥塞避免</h3><p>发送方维持一个叫做<strong>拥塞窗口cwnd</strong>（congestion window）的变量。发送方控制拥塞窗口的原则是：只要网络没有发送拥塞，发送窗口就大些，以便把更多分组发送出去。网络发生拥塞时，窗口就减少一些，减少注入到网络的分组。</p>
<p>发送方如何知道网络发生了拥塞呢？我们知道，当网络发生拥塞时，路由器就要丢弃分组，因此只要发送方没有按时收到应当到达的确认报文，就可以猜想网络可能发生了拥塞。</p>
<p>拥塞窗口大小如何变化？先从慢开始算法开始讲起。</p>
<p><strong>慢开始算法思路是这样的</strong>。当主机开始发送数据时，如果立即把大量数据字节注入到网络，那么就有可能引起网络拥塞，因为现在还不知道网络的负荷情况。经验表明，较好的方法是先探测一下，即由小到大逐渐增大发送窗口，也就是说，<strong>由小到大逐渐增大拥塞窗口数值</strong>。通常在刚刚开始发送报文段时，先把拥塞窗口cwnd设置为一个最大报文段MSS的数值。后面每收到一个对新的报文段的确认后，把拥塞窗口增加至多一个MSS的数值。</p>
<p>如下图，发送发方每收到一个确认，拥塞窗口就加1。<strong>每经过一个传输轮次（transmission round），拥塞窗口cwnd就加倍</strong>。</p>
<img src="/2024/09/08/%E4%BC%A0%E8%BE%93%E6%8E%A7%E5%88%B6%E5%8D%8F%E8%AE%AETCP/%E6%85%A2%E5%BC%80%E5%A7%8B.png" class="" title="慢开始">

<p>为了防止拥塞窗口cwnd增长过大引起网络拥塞，还需要设置一个<strong>慢开始门限</strong>ssthresh状态变量。慢开始门限ssthresh用法如下：</p>
<p>当cwnd&lt;ssthresh时，使用上述慢开始算法。</p>
<p>当cwnd&gt;ssthresh时,停止使用慢开始算法而改用拥塞避免算法。</p>
<p>当cwnd&#x3D;ssthresh时，既可以使用慢开始算法，也可以使用拥塞避免算法。</p>
<p><strong>拥塞避免算法的思路</strong>是让拥塞窗口cwnd缓慢地增大，即每经过一个往返时间RTT就把发送方的拥塞窗口cwnd加1，而不是加倍。这样拥塞窗口cwnd按线性规律缓慢增长，比慢开始算法的拥塞窗口增长速率缓慢很多。</p>
<p>无论在慢开始阶段还是在拥塞避免阶段，只要发送方判断网络出现拥塞（判断根据就是没有按时收到确认），就要把慢开始门限ssthresh设置为出现拥塞时发送窗口值的一半（但不能小于2）。然后把拥塞窗口cwnd重新设置为1，执行慢开始算法。这样做的目的就是要迅速减少主机发送到网络中的分组数，使得发生拥塞的路由器有足够的时间把队列中积压的分组处理完毕。</p>
<p>下图中，举例说明慢开始和拥塞避免。可以看到，<strong>乘法减小</strong>（Multiplicative Decrease）和<strong>加法增大</strong>（Additive Increase）的提法，这两种算法合在一起常称为<strong>AIMD算法</strong>（加法增大乘法减小）。</p>
<img src="/2024/09/08/%E4%BC%A0%E8%BE%93%E6%8E%A7%E5%88%B6%E5%8D%8F%E8%AE%AETCP/%E6%85%A2%E5%BC%80%E5%A7%8B%E5%92%8C%E6%8B%A5%E5%A1%9E%E9%81%BF%E5%85%8D.png" class="" title="慢开始和拥塞避免">

<h3 id="快重传和快恢复"><a href="#快重传和快恢复" class="headerlink" title="快重传和快恢复"></a>快重传和快恢复</h3><p><strong>快重传算法</strong>首先要求接收方每收到一个<strong>失序的报文段</strong>后就立即发出重复确认（为的是使发送方及早知道有报文段没有到达对方）而不要等到自己发送数据时才捎带确认。</p>
<p>如下图，接收方收到了M1和M2，都分别发出了确认。现在接收方收到了M4，但是没有收到M3，显然不能对M4进行确认。按照可靠性传输原理，接收方可以什么都不做，也可以在适当时机发送一次对M2的确认。但是按照快重传算法的规定，接收方应及时发出对M2的重复确认，这样做可以让发送方及时知道M3没有被确认。发送方接着发送M5和M6。接收方收到后，也还要再次对M2发出重复确认。快重传规定，发送方只要<strong>一连收到三次重复确认</strong>就应当立即重传对方尚未收到的报文段M3，而不必等到M3设置得重传计时器结束。快重传可以使整个网络的吞吐量提高约20%。</p>
<img src="/2024/09/08/%E4%BC%A0%E8%BE%93%E6%8E%A7%E5%88%B6%E5%8D%8F%E8%AE%AETCP/%E5%BF%AB%E9%87%8D%E4%BC%A0.png" class="" title="快重传">

<p>和快重传配合使用的还有<strong>快恢复</strong>算法，其过程主要有一下两点：</p>
<p>（1）当发送方连续收到三个重复确认时，就执行“乘法减小”算法，把慢开始门限ssthresh减半。这是为了预防网络发生拥塞。请注意，接下去不执行慢开始算法。</p>
<p>（2）由于发送方现在任务网络很可能没有发生拥塞（如果发送严重拥塞，就不会一连有好几个报文段连续到达接受方，就不会导致接收方连续发送重复确认），因此与慢开始不同，现在不执行慢开始（即cwnd现在不设置为1），而是把cwnd设置为慢开始ssthresh门限减半后的数值，然后开始执行拥塞避免算法，使得拥塞窗口线性增大。</p>
<p>下图给出了快重传和快恢复的示意图，并标明了“TCP Reno版本”，这是目前使用得很广泛的版本。图中还画出了已经废弃不用的虚线部分（TCP Tahoe版本）。请注意它们的区别就是：新的TCP Reno版本在快重传之后采用快恢复算法而不是采用慢开始算法。</p>
<img src="/2024/09/08/%E4%BC%A0%E8%BE%93%E6%8E%A7%E5%88%B6%E5%8D%8F%E8%AE%AETCP/%E5%BF%AB%E9%87%8D%E4%BC%A0%E6%8B%A5%E5%A1%9E%E9%81%BF%E5%85%8D.png" class="" title="快重传拥塞避免">

<p>在采用快恢复算法时，慢开始算法只是在TCP 连接建立时和网络出现超时时才使用。</p>
<p>我们这节在讨论几种拥塞算法时，都假定了接收方总是有足够大的缓存空间，因而发送窗口的大小由拥塞窗口决定。实际上，接收方的缓存窗口总是有限的。因此需要拥塞窗口和接收窗口一起考虑，那么很显然，发送方的窗口的上限值应当取为接收方窗口rwnd和拥塞窗口cwnd这两个变量中较小的一个：</p>
<p>发送窗口的上限值&#x3D;Min [rwnd, cwnd]</p>
<p>当rwnd &lt; cwnd时，是接收方的接收能力限制发送方窗口的最大值。反之，当cwnd &lt; rwnd时，则是网络的拥塞限制发送方窗口的最大值。也就是说， rwnd和cwnd中较小的一个控制发送方发送数据的速率。</p>
<h1 id="TCP运输连接管理"><a href="#TCP运输连接管理" class="headerlink" title="TCP运输连接管理"></a>TCP运输连接管理</h1><p>TCP是面向连接的协议。运输连接有三个阶段：<strong>连接建立、数据传送和连接释放</strong>。</p>
<p>在TCP连接建立过程中要解决以下三个问题：</p>
<p>（1）要使每一方能够确知对方的存在。</p>
<p>（2）要允许双方协商一些参数（如最大窗口值、是否使用窗口扩大选项、时间戳选项、服务质量等）。</p>
<p>（3）能够对运输实体资源（如缓存大小、连接表中的项目等）进行分配。</p>
<p>TCP连接的建立采用客户服务器方式。主动发起建立请求的应用进程叫做客户（client）。被动等待连接建立的应用进程叫做服务器（server）。</p>
<h2 id="TCP的连接建立"><a href="#TCP的连接建立" class="headerlink" title="TCP的连接建立"></a>TCP的连接建立</h2><p>下图是TCP的建立连接过程。假定主机A是TCP客户程序，主机B运行TCP服务器程序。</p>
<img src="/2024/09/08/%E4%BC%A0%E8%BE%93%E6%8E%A7%E5%88%B6%E5%8D%8F%E8%AE%AETCP/%E4%B8%89%E6%AC%A1%E6%8F%A1%E6%89%8B%E5%BB%BA%E7%AB%8BTCP%E8%BF%9E%E6%8E%A5.png" class="" title="三次握手建立TCP连接">

<p>B的TCP服务进程先创建<strong>传输控制块TCB</strong>(Transmission Control Block)，准备接受客户进程的连接请求。然后服务器进程进入LISTEN(收听)状态。</p>
<p>A的TCP客户进程也是首先创建TCB，然后向B发出连接请求报文段。这时首部中的同步为SYN&#x3D;1，同时选择一个初始序号seq&#x3D;x。TCP规定，SYN报文段（即SYN&#x3D;1的报文段）不能携带数据，但是<strong>需要消耗一个序号</strong>。这时，TCP客户程序进入SYN-SENT（同步以发送）状态。</p>
<p>B收到连接请求报文段后，如同意建立连接，则向A发送确认。在确认报文段中，把SYN和ACK位都置为1，确认号ack&#x3D;x+1，同时自己也选择一个序号y。需要注意，这个报文段不能携带数据，但是同样要消耗掉一个序号。这时，TCP服务器进程进入SYN-RCVD（同步收到）状态。</p>
<p>A收到B的确认后，还要向B给出确认。确认报文段的ACK置为1，确认号ack&#x3D;y+1，而自己的序号seq&#x3D;x+1。TCP规定，ACK报文段可以携带数据。但<strong>如果不携带数据则不消耗序号</strong>，在这种情况下，下一个数据报文段的序号仍是seq&#x3D;x+1。这时，TCP连接已经建立，A进入ESTABLISHED（已建立连接）状态。</p>
<p>B收到A的确认后，也进入ESTABLISHED（已建立连接）状态。</p>
<p>上面给出的连接建立过程叫做<strong>三次握手</strong>（three-way handshake）。</p>
<p><strong>为什么A还要发送一次确认呢？这主要是防止已失效的连接请求报文段突然又传送到B，因而产生错误。</strong>这个失效的报文段请求可能是这样产生的，由于该报文段在网络结点长时间滞留了，以致到连接释放后，才在某个时间点到达B。本来这时一个失效的报文段，B误认为A又发起了新的请求，假如不是三次握手，B就接受连接了，这样B的资源就白白浪费了。</p>
<h2 id="TCP的连接释放"><a href="#TCP的连接释放" class="headerlink" title="TCP的连接释放"></a>TCP的连接释放</h2><p>数据传输结束后，通信的双方都可释放连接。如下图，现在A和B都处于ESTABLISHED状态。</p>
<p>A应用进程先向其TCP发出连接释放报文段，并停止发送数据，主动关闭TCP连接。A把报文段的终止控制位置为1，FIN&#x3D;1，其序号seq&#x3D;u，它等于前面传送过的数据的最后一个字节序号加1。这时A进入FIN-WAIT-1（终止等待1）状态，等待B确认。TCP规定，FIN报文段即使不携带数据也要消耗一个序号。</p>
<img src="/2024/09/08/%E4%BC%A0%E8%BE%93%E6%8E%A7%E5%88%B6%E5%8D%8F%E8%AE%AETCP/TCP%E8%BF%9E%E6%8E%A5%E9%87%8A%E6%94%BE%E8%BF%87%E7%A8%8B.png" class="" title="TCP连接释放过程">

<p>B收到连接释放报文段后，发出确认，确认号是ack&#x3D;u+1，而这个报文段自己的序号是seq&#x3D;v，等于B前面已经传送过的数据的最后一个字节的序号加1。然后B进入CLOSE-WAIT（关闭等待）状态。TCP服务器进程这时应通知高层应用进程，因而从A到B这个方向的连接就释放了，这时的TCP连接处于<strong>半关闭</strong>（half-close）状态，即A没有数据发送给B了，但若B仍要发送数据给A，A仍要接收。B到A的方向并没有关闭，这个状态可能会持续一段时间。</p>
<p>A收到来自B的确认后，就进入FIN-WAIT-2（终止等待2）状态，等待B发出的连接释放报文段。</p>
<p>若B已经没有要向A发送的数据，其应用进程就通知TCP释放连接。这时B发出的报文段必须FIN&#x3D;1。现假定B的序号为w（半关闭状态B可能又发送了一些数据）。B还必须重复上次发送过的确认号ack&#x3D;u+1。这时B就进入LAST-ACK（最后确认）状态，等待A确认。</p>
<p>A收到B的连接释放报文段后，必须对此发出确认。在确认报文段的ACK&#x3D;1，确认号ack&#x3D;w+1，序号是seq&#x3D;u+1（根据TCP标准，前面发送过的FIN报文段要消耗一个序号）。然后进入到TIME-WAIY（时间等待）状态。请注意，现在TCP连接还没有释放掉。必须经过<strong>时间等待计时器</strong>（TIME-WAIT timer）设置的时间2MSL后，A才进入到CLOSED状态。<strong>时间MSL叫做最长报文段寿命</strong>（Maximum Segment Lifetime），RFC793建议设为2分钟（可根据实际情况设置小一点）。因此，A进入TIME-WAIT状态后，要经过4分钟才能进入CLOSED状态，才能开始建立下一个新的连接。当A撤销响应的TCB后，就结束了这次的TCP连接。</p>
<p>为什么A在TIME-WAIT状态必须等待2MSL的时间呢？有两个理由：</p>
<p>（1）为了保证A发送的最后一个ACK报文段能够到达B。这个ACK报文段有可能丢失，因而使处在LAST-ACK状态的B收不到对已发送的FIN + ACK报文段的确认。B会超时重传这个FIN + ACK报文段，而A就能在2MSL时间内收到这个重传的FIN + ACK报文段。接着A重传一次确认，重新启动2MSL计时器。最后，A和B都正常进入到CLOSED状态。如果A在TIME-WAIT状态不等待一段时间，而是在发送完ACK报文段后立即释放连接，那么就无法收到B重传的FIN + ACK报文段，因而也不会再发送一次确认报文段。这样，B就无法按照正常步骤进入CLOSED状态。</p>
<p>（2）防止前面提到的“已失效的连接请求报文段”出现在本连接中。B只要收到了A发出的确认，就进入CLOSED状态。同样，B在撤销相应的传输控制块TCB后，就结束了这次的TCP连接。我们注意到，B结束TCP连接的时间要比A早一些。上述的TCP连接释放过程是四次握手。</p>
<p>B只要收到了A发出的确认，就进入CLOSED状态。同样，B在撤销相应的传输控制块TCB后，就结束了这次的TCP连接。我们注意到，B结束TCP连接的时间要比A早一些。</p>
<p>上述的TCP连接释放过程是四次握手。</p>
<h2 id="TCP的有限状态机"><a href="#TCP的有限状态机" class="headerlink" title="TCP的有限状态机"></a>TCP的有限状态机</h2><p>下图给出了TCP的有限状态机。图中每一个方框即TCP可能具有的状态。</p>
<p>每个方框中的大写英文字符串是TCP标准所使用的TCP连接状态名。状态之间的箭头表示可能发生的状态变迁。</p>
<p>箭头旁边的字，表明引起这种变迁的原因，或表明发生状态变迁后又出现什么动作。</p>
<p>请注意图中有三种不同的箭头。粗实线箭头表示对客户进程的正常变迁。粗虚线箭头表示对服务器进程的正常变迁。另一种细线箭头表示异常变迁。</p>
<img src="/2024/09/08/%E4%BC%A0%E8%BE%93%E6%8E%A7%E5%88%B6%E5%8D%8F%E8%AE%AETCP/TCP%E6%9C%89%E9%99%90%E7%8A%B6%E6%80%81%E6%9C%BA.png" class="" title="TCP有限状态机">

]]></content>
      <tags>
        <tag>网络</tag>
      </tags>
  </entry>
</search>
